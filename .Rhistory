title <- ggdraw() + drawlabel("Distribution of Nutrients by Cluster Membership in the MammalsMilk Data Set")
plot_grid(graph1, graph2, graph3, graph4, graph5, labels = "AUTO", title)
#install.packages("cowplot")
install.packages("gridGraphics")
plot_grid(graph1, graph2, graph3, graph4, graph5, labels = "AUTO", title)
title <- ggdraw() + drawlabel("Distribution of Nutrients by Cluster Membership in the MammalsMilk Data Set")
plot_grid(graph1, graph2, graph3, graph4, graph5, labels = "AUTO", title)
title <- ggdraw() + drawlabel("Distribution of Nutrients by Cluster Membership in the MammalsMilk Data Set")
title <- ggdraw() + draw_label("Distribution of Nutrients by Cluster Membership in the MammalsMilk Data Set")
plot_grid(graph1, graph2, graph3, graph4, graph5, labels = "AUTO", title)
plot_grid(graph1, graph2, graph3, graph4, graph5, labels = "AUTO", title, ncol=1, rel_heights=c(0.1, 1))
plot_grid(graph1, graph2, graph3, graph4, graph5, labels = "AUTO", title, rel_heights=c(0.1, 1))
plot_grid(graph1, graph2, graph3, graph4, graph5, labels = "AUTO")
Cluster_Ex<-read.csv("") # The path to your data goes here.
head(Cluster_Ex)
Kmeans_3<-kmeans(Cluster_Ex,centers=3)
Cluster_Ex<-read.csv("Cluster_Ex") # The path to your data goes here.
Cluster_Ex<-read.csv("C:/Users/Peter/Documents/College Documents/DATA 180/Data 180 Peter Guma/DATA-180-PETER-GUMA/Cluster_Ex") # The path to your data goes here.
Cluster_Ex<-read.csv("C:/Users/Peter/Documents/College Documents/DATA 180/Data 180 Peter Guma/DATA-180-PETER-GUMA/Cluster_Ex.csv") # The path to your data goes here.
head(Cluster_Ex)
Kmeans_3<-kmeans(Cluster_Ex,centers=3)
#Centers.
Kmeans_3$centers
Kmeans_3
#Centers.
Kmeans_3$centers
#Within Group Sum of Squares.
Kmeans_3$withinss
#Clusters.
Kmeans_3$cluster
#Centers.
sum(Kmeans_3$centers)
#Centers.
Kmeans_3$centers
#Within Group Sum of Squares.
Kmeans_3$withinss
#Clusters.
Kmeans_3$cluster
head(Cluster_Ex)
Kmeans_3<-kmeans(Cluster_Ex,centers=3)
Kmeans_3
#Centers.
Kmeans_3$centers
#Within Group Sum of Squares.
Kmeans_3$withinss
#Clusters.
Kmeans_3$cluster
# Plotting clusters.
plot(X2~X1,data=Cluster_Ex,xlim=c(0,4),ylim=c(0,4),cex.axis=1.3, cex.lab=1.2,cex=1.2,pch=15+Kmeans_3$cluster,col=Kmeans_3$cluster)
#Adding centroids.
points(X2~X1,data=Kmeans_3$centers,pch=10,cex=1.8,col="blue")
Kmeans_3<-kmeans(Cluster_Ex,centers=5)
Kmeans_3
#Centers.
Kmeans_3$centers
Kmeans_3
#Centers.
Kmeans_3$centers
#Within Group Sum of Squares.
Kmeans_3$withinss
#Clusters.
Kmeans_3$cluster
# Plotting clusters.
plot(X2~X1,data=Cluster_Ex,xlim=c(0,4),ylim=c(0,4),cex.axis=1.3, cex.lab=1.2,cex=1.2,pch=15+Kmeans_3$cluster,col=Kmeans_3$cluster)
Kmeans_3<-kmeans(Cluster_Ex,centers=10)
Kmeans_3
#Centers.
Kmeans_3$centers
#Within Group Sum of Squares.
Kmeans_3$withinss
#Clusters.
Kmeans_3$cluster
# Plotting clusters.
plot(X2~X1,data=Cluster_Ex,xlim=c(0,4),ylim=c(0,4),cex.axis=1.3, cex.lab=1.2,cex=1.2,pch=15+Kmeans_3$cluster,col=Kmeans_3$cluster)
#Adding centroids.
points(X2~X1,data=Kmeans_3$centers,pch=10,cex=1.8,col="blue")
Kmeans_3<-kmeans(Cluster_Ex,centers=3)
Kmeans_3
#Centers.
Kmeans_3$centers
#Within Group Sum of Squares.
Kmeans_3$withinss
#Clusters.
Kmeans_3$cluster
# Plotting clusters.
plot(X2~X1,data=Cluster_Ex,xlim=c(0,4),ylim=c(0,4),cex.axis=1.3, cex.lab=1.2,cex=1.2,pch=15+Kmeans_3$cluster,col=Kmeans_3$cluster)
#Adding centroids.
points(X2~X1,data=Kmeans_3$centers,pch=10,cex=1.8,col="blue")
palette()
Kmeans_5_1<-kmeans(Cluster_Ex,centers=5)
plot(X2~X1,data=Cluster_Ex,xlim=c(0,4),ylim=c(0,4),cex.axis=1.3, cex.lab=1.2,cex=1.2,pch=15+Kmeans_5_1$cluster,col=Kmeans_5_1$cluster)
Kmeans_5_2<-kmeans(Cluster_Ex,centers=5)
plot(X2~X1,data=Cluster_Ex,xlim=c(0,4),ylim=c(0,4),cex.axis=1.3, cex.lab=1.2,cex=1.2,pch=15+Kmeans_5_2$cluster,col=Kmeans_5_2$cluster)
Kmeans_5_1<-kmeans(Cluster_Ex,centers=5)
plot(X2~X1,data=Cluster_Ex,xlim=c(0,4),ylim=c(0,4),cex.axis=1.3, cex.lab=1.2,cex=1.2,pch=15+Kmeans_5_1$cluster,col=Kmeans_5_1$cluster)
Kmeans_5_2<-kmeans(Cluster_Ex,centers=5)
plot(X2~X1,data=Cluster_Ex,xlim=c(0,4),ylim=c(0,4),cex.axis=1.3, cex.lab=1.2,cex=1.2,pch=15+Kmeans_5_2$cluster,col=Kmeans_5_2$cluster)
#using set seed for reproducibility.
Kmeans_5_3<-kmeans(Cluster_Ex,centers=Cluster_Ex[c(2,5,7,21,36),])
set.seed(5)
Kmeans_5_4<-kmeans(Cluster_Ex,centers=5)
plot(X2~X1,data=Cluster_Ex,xlim=c(0,4),ylim=c(0,4),cex.axis=1.3, cex.lab=1.2,cex=1.2,pch=15+Kmeans_5_4$cluster,col=Kmeans_5_4$cluster)
set.seed(6)
Kmeans_5_4<-kmeans(Cluster_Ex,centers=5)
plot(X2~X1,data=Cluster_Ex,xlim=c(0,4),ylim=c(0,4),cex.axis=1.3, cex.lab=1.2,cex=1.2,pch=15+Kmeans_5_4$cluster,col=Kmeans_5_4$cluster)
# Clustering with 3 clusters, nstart = 1. Randomize only once.
Cluster3_S1<-kmeans(Cluster_Ex,centers=3)
Cluster3_S50<-kmeans(Cluster_Ex,centers=3,nstart=50)
Cluster3_S1
Cluster3_S1$withinss
set.seed(6)
#using set seed for reproducibility.
Kmeans_5_3<-kmeans(Cluster_Ex,centers=Cluster_Ex[c(2,5,7,21,36),])
set.seed(6)
Kmeans_5_4<-kmeans(Cluster_Ex,centers=5)
plot(X2~X1,data=Cluster_Ex,xlim=c(0,4),ylim=c(0,4),cex.axis=1.3, cex.lab=1.2,cex=1.2,pch=15+Kmeans_5_4$cluster,col=Kmeans_5_4$cluster)
set.seed(5)
Kmeans_5_4<-kmeans(Cluster_Ex,centers=5)
plot(X2~X1,data=Cluster_Ex,xlim=c(0,4),ylim=c(0,4),cex.axis=1.3, cex.lab=1.2,cex=1.2,pch=15+Kmeans_5_4$cluster,col=Kmeans_5_4$cluster)
# Clustering with 3 clusters, nstart = 1. Randomize only once.
Cluster3_S1<-kmeans(Cluster_Ex,centers=3)
Cluster3_S50<-kmeans(Cluster_Ex,centers=3,nstart=50)
Cluster3_S1
Cluster3_S1$withinss
Cluster3_S1$tot.withinss
# Clustering with 3 clusters, nstart =50. Randomize 50 times.
Cluster3_S50
Cluster3_S50$withinss
Cluster3_S50$tot.withinss
Cluster3_S50$betweenss
# WGSS as a function of the number of clusters for values of K from 1 to 8.
wgss<-rep(0,8)
for(i in 1:8){wgss[i]<-kmeans(Cluster_Ex,centers=i,nstart=50)$tot.withinss}
plot(c(1:8),wgss,type="b",pch=16,cex=1.3,ylim=c(0,100), xlab="Number of Groups",ylab="WGSS")
# Clustering with 3 clusters, nstart =50. Randomize 50 times.
Cluster3_S50
Cluster3_S50$withinss
Cluster3_S50$tot.withinss
Cluster3_S50$betweenss
# WGSS as a function of the number of clusters for values of K from 1 to 8.
wgss<-rep(0,8)
for(i in 1:8){wgss[i]<-kmeans(Cluster_Ex,centers=i,nstart=50)$tot.withinss}
plot(c(1:8),wgss,type="b",pch=16,cex=1.3,ylim=c(0,100), xlab="Number of Groups",ylab="WGSS")
Kmeans_3$betweenss
# Clustering with 3 clusters, nstart = 1. Randomize only once.
Cluster3_S1<-kmeans(Cluster_Ex,centers=3)
Cluster3_S50<-kmeans(Cluster_Ex,centers=3,nstart=50)
Cluster3_S1
Cluster3_S1$withinss
Cluster3_S1$tot.withinss
# Clustering with 3 clusters, nstart =50. Randomize 50 times.
Cluster3_S50
Cluster3_S50$withinss
Cluster3_S50$tot.withinss
Cluster3_S50$betweenss
# WGSS as a function of the number of clusters for values of K from 1 to 8.
wgss<-rep(0,8)
for(i in 1:8){wgss[i]<-kmeans(Cluster_Ex,centers=i,nstart=50)$tot.withinss}
plot(c(1:8),wgss,type="b",pch=16,cex=1.3,ylim=c(0,100), xlab="Number of Groups",ylab="WGSS")
Kmeans_3$betweenss
# CH index.
chindex=c() # or rep(0,7)
# CH index.
chindex=c() # or rep(0,7)
i=1
for(k in 2:8){
chindex[i] <- (kmeans(df[2:3],k)$betweenss/(k-1))  / (kmeans(df[2:3],k)$tot.withinss/((nrow(df)-k)))
i=i+1
}
# CH index.
chindex=c() # or rep(0,7)
# CH index.
chindex=c() # or rep(0,7)
i=1
for(k in 2:8){
chindex[i] <- (kmeans(df[2:3],k)$betweenss/(k-1))  / (kmeans(df[2:3],k)$tot.withinss/((nrow(df)-k)))
i=i+1
}
plot(2:8,chindex,pch=19,type='b')
for(i in 1:8){wgss[i]<-kmeans(Cluster_Ex,centers=i,nstart=50)$tot.withinss}
# WGSS as a function of the number of clusters for values of K from 1 to 8.
wgss<-rep(0,8)
for(i in 1:8){wgss[i]<-kmeans(Cluster_Ex,centers=i,nstart=50)$tot.withinss}
plot(c(1:8),wgss,type="b",pch=16,cex=1.3,ylim=c(0,100), xlab="Number of Groups",ylab="WGSS")
setwd("C:/Users/Peter/Documents/College Documents/DATA 180/Data 180 Peter Guma") # Set this to your local GitHub repository.
library(tm) # text mining package
# Instead of that, just read in the speech
charVector <- scan("speech.txt", character(0), sep = "\n") # "\n is a line separator
head(charVector)
setwd("C:/Users/Peter/Documents/College Documents/DATA 180/Data 180 Peter Guma") # Set this to your local GitHub repository.
library(tm) # text mining package
# Instead of that, just read in the speech
charVector <- scan("speech.txt", character(0), sep = "\n") # "\n is a line separator
# Instead of that, just read in the speech
charVector <- scan("speech.txt", character(0), sep = "\n") # "\n is a line separator
setwd("C:/Users/Peter/Documents/College Documents/DATA 180/Data 180 Peter Guma") # Set this to your local GitHub repository.
library(tm) # text mining package
# Instead of that, just read in the speech
charVector <- scan("speech.txt", character(0), sep = "\n") # "\n is a line separator
head(charVector)
charVector_block <- scan("C:/Users/Peter/Documents/College Documents/DATA 180/Data 180 Peter Guma/speech.txt", character(0))
charVector_block # default separator for the scan function is space.
# Instead of that, just read in the speech
charVector <- scan("C:/Users/Peter/Documents/College Documents/DATA 180/Data 180 Peter Guma/speech.txt", character(0), sep = "\n") # "\n is a line separator
head(charVector)
setwd("C:/Users/Peter/Documents/College Documents/DATA 180/Data 180 Peter Guma/DATA-180-PETER-GUMA") # Set this to your local GitHub repository.
library(tm) # text mining package
# Instead of that, just read in the speech
charVector <- scan("speech.txt", character(0), sep = "\n") # "\n is a line separator
head(charVector)
setwd("C:/Users/Peter/Documents/College Documents/DATA 180/Data 180 Peter Guma/DATA-180-PETER-GUMA") # Set this to your local GitHub repository.
library(tm) # text mining package
# Instead of that, just read in the speech
charVector <- scan("speech.txt", character(0), sep = "\n") # "\n is a line separator
head(charVector)
charVector_block <- scan("speech.txt", character(0))
charVector_block # default separator for the scan function is space.
posWords <- scan("positive-words.txt", character(0), sep = "\n")  # 2006 items
negWords <- scan("negative-words.txt", character(0), sep = "\n")  # 4783 items
head(posWords,15)
head(negWords,15)
head(posWords,15)
head(negWords,15)
# cleaning part!!
wordVector <- VectorSource(charVector) # from tm package, convert to vector
class(wordVector); typeof(wordVector); length(wordVector)
wordCorpus <- Corpus(wordVector) # convert to corpus
class(wordCorpus); typeof(wordCorpus); length(wordCorpus)
# first step transformation: make all of the letters in "wordCorpus" lowercase
wordCorpus <- tm_map(wordCorpus, content_transformer(tolower))
# second step transformation: remove the punctuation in "wordCorpus"
wordCorpus <- tm_map(wordCorpus, removePunctuation)
# third step transformation: remove numbers in "wordCorpus"
wordCorpus <- tm_map(wordCorpus, removeNumbers)
# final step transformation: take out the "stop" words, such as "the", "a" and "at"
wordCorpus <- tm_map(wordCorpus, removeWords, stopwords("english"))
stop_words<-stopwords("english")
stop_words
setwd("C:/Users/Peter/Documents/College Documents/DATA 180/Data 180 Peter Guma/DATA-180-PETER-GUMA") # Set this to your local GitHub repository.
library(tm) # text mining package
# Instead of that, just read in the speech
charVector <- scan("speech.txt", character(0), sep = "\n") # "\n is a line separator
# Instead of that, just read in the speech
charVector <- scan("speech.txt", character(0), sep = "\n") # "\n is a line separator
head(charVector)
# Instead of that, just read in the speech
charVector <- scan("speech.txt", character(0)) # "\n is a line separator
head(charVector)
setwd("C:/Users/Peter/Documents/College Documents/DATA 180/Data 180 Peter Guma/DATA-180-PETER-GUMA") # Set this to your local GitHub repository.
library(tm) # text mining package
# Instead of that, just read in the speech
charVector <- scan("speech.txt", character(0), sep = "\n") # "\n is a line separator
head(charVector)
# Instead of that, just read in the speech
charVector <- scan("speech.txt", character(0), sep = "\n") # "\n is a line separator
head(charVector)
charVector_block <- scan("speech.txt", character(0))
charVector_block # default separator for the scan function is space.
posWords <- scan("positive-words.txt", character(0), sep = "\n")  # 2006 items
negWords <- scan("negative-words.txt", character(0), sep = "\n")  # 4783 items
head(posWords,15)
head(negWords,15)
# cleaning part!!
wordVector <- VectorSource(charVector) # from tm package, convert to vector
class(wordVector); typeof(wordVector); length(wordVector)
wordCorpus <- Corpus(wordVector) # convert to corpus
class(wordCorpus); typeof(wordCorpus); length(wordCorpus)
# first step transformation: make all of the letters in "wordCorpus" lowercase
wordCorpus <- tm_map(wordCorpus, content_transformer(tolower))
# second step transformation: remove the punctuation in "wordCorpus"
wordCorpus <- tm_map(wordCorpus, removePunctuation)
# third step transformation: remove numbers in "wordCorpus"
wordCorpus <- tm_map(wordCorpus, removeNumbers)
# final step transformation: take out the "stop" words, such as "the", "a" and "at"
wordCorpus <- tm_map(wordCorpus, removeWords, stopwords("english"))
stop_words<-stopwords("english")
stop_words
stop_words<-stopwords("russian")
stop_words
stop_words<-stopwords("esperanto")
stop_words
stop_words<-stopwords("esperanto")
stop_words<-stopwords("latin")
stop_words<-stopwords("english")
stop_words
stop_words<-stopwords("english")
stop_words
wordCorpus[["1"]][["content"]] # Review what's left of the first paragraph
# create a term-document matrix "tdm"
tdm <- TermDocumentMatrix(wordCorpus)
# view term-document matrix "tdm"
tdm
fTerms <- findFreqTerms(tdm, lowfreq = 20) # from tm
fTerms
# Common terms: "tonight"  "new"      "people"   "children" "family"   "will"     "families" "america"
findAssocs(tdm, fTerms, 0.4) # for fTerms, what comes next?
Zipf_plot(tdm)
# Create a list of counts for each word
# convert tdm into a matrix called "m"
m <- as.matrix(tdm)
dim(m) # 1210 x 166, 1210 unique words
# get total count for each word
wordCounts <- rowSums(m)
# sort words in "wordCounts" by frequency
wordCounts <- sort(wordCounts, decreasing=TRUE)
# check the first several items in "wordCounts" to see if it is built correctly
head(wordCounts)
totalWords <- sum(wordCounts) # total occurance of words
barplot(wordCounts)
barplot(table(wordCounts))
# identify positive words
matchedP <- match(names(wordCounts), posWords, nomatch = 0)
matchedP <- wordCounts[matchedP != 0]
barplot(matchedP,las=2,cex.names=0.75)
sum(matchedP)/totalWords
# Exercise: Generate the Negative Word Matches and Interpret Results
matchedN <- match(names(wordCounts), negWords, nomatch = 0)
str(matchedN); head(matchedN,50) # What is matchedN right now?
matchedN <- matchedN != 0 # A boolean map
str(matchedN); head(matchedN,50) # How about now?
matchedN <- wordCounts[matchedN] # Use matchedN as a selector
str(matchedN); head(matchedN,50) # And finally, what about now?
barplot(matchedN,las=2,cex.names=0.75)
sum(matchedN)/totalWords
#install.packages("spacyr") # should only need to do this once.
library("spacyr")
spacy_install()
#if things give error, or get stuck, try
library("reticulate")
reticulate::conda_install(envname = "spacy_condaenv", packages = 'spacy')
spacy_install()
library("spacyr")
spacy_initialize() # Start the spacy session
startTime <- Sys.time()
outTokens <- spacy_parse(inText)
Sys.time() - startTime # Takes 0.36 seconds, or about 0.18 seconds per sentence
outTokens
outTokens2 <- spacy_parse(inText, lemma = FALSE)
entity_extract(outTokens2) # get person
############################################################################################
# national anthems
library(tm)
# Read in anthems
anthems <- read.csv('https://raw.githubusercontent.com/lucas-de-sa/national-anthems-clustering/master/datasets/anthems.csv')
charVector <- anthems$Anthem
head(charVector)
posWords <- scan("positive-words.txt", character(0), sep = "\n")  # 2006 items
negWords <- scan("negative-words.txt", character(0), sep = "\n")  # 4783 items
head(posWords,15)
head(negWords,15)
wordVector <- VectorSource(charVector) # converts it into a vector
class(wordVector); typeof(wordVector); length(wordVector)
wordCorpus <- Corpus(wordVector) # converts it into a corpus
class(wordCorpus); typeof(wordCorpus); length(wordCorpus) # handy for cleaning.
# cleaning part!!
# first step transformation: make all of the letters in "wordCorpus" lowercase
wordCorpus <- tm_map(wordCorpus, content_transformer(tolower))
# second step transformation: remove the punctuation in "wordCorpus"
wordCorpus <- tm_map(wordCorpus, removePunctuation)
# third step transformation: remove numbers in "wordCorpus"
wordCorpus <- tm_map(wordCorpus, removeNumbers)
# final step transformation: take out the "stop" words, such as "the", "a" and "at"
wordCorpus <- tm_map(wordCorpus, removeWords, stopwords("english"))
wordCorpus[["1"]][["content"]] # Review what's left of the first paragraph
# create a term-document matrix "tdm"
tdm <- TermDocumentMatrix(wordCorpus)
# view term-document matrix "tdm"
tdm
# i is the index number of the term in question, j is what document it belongs to
# v how many times the term occurs in the respective document
# A list of document names and terms is under dimnames
m <- as.matrix(tdm)
# create a list of counts for each word named "wordCounts"
wordCounts <- rowSums(m)
# sort words in "wordCounts" by frequency
wordCounts <- sort(wordCounts, decreasing=TRUE)
# check the first several items in "wordCounts" to see if it is built correctly
head(wordCounts)
totalWords <- sum(wordCounts)
barplot(wordCounts[wordCounts>50],cex.names=0.95) # most popular words
barplot(table(wordCounts),las=2,cex.names=0.75) # frequency of occurences per anthem
# identify positive words
matchedP <- match(names(wordCounts), posWords, nomatch = 0)
matchedP <- matchedP != 0
matchedP <- wordCounts[matchedP]
str(matchedP); head(matchedP,50) # Positive
barplot(matchedP,las=2,cex.names=0.75)
sum(matchedP)/totalWords # 17% positive words!
# Exercise: Generate the Negative Word Matches and Interpret Results
matchedN <- match(names(wordCounts), negWords, nomatch = 0)
str(matchedN); head(matchedN,50) # What is matchedN right now?
matchedN <- matchedN != 0 # A boolean map
str(matchedN); head(matchedN,50) # How about now?
matchedN <- wordCounts[matchedN] # Use matchedN as a selector
str(matchedN); head(matchedN,50) # And finally, what about now?
barplot(matchedN,las=2,cex.names=0.75)
sum(matchedN)/totalWords # 5% negative words
library("quanteda") # Put the package in memory
library("corpus")
# let's use corpus package, alternative to tm package
term_stats(charVector, ngrams = 1) # get term stats, for single word. Can change ngrams.
term_stats(charVector, ngrams = 1,filter = text_filter(drop_punct = TRUE, drop_symbol = TRUE, drop = stopwords_en))
# Convert to corpus, reshape to paragraphs
anthemcorpus <- corpus(charVector, docnames=anthems$country)
paras <- corpus_reshape(anthemcorpus, to="paragraphs")
anthem_dtm <- dfm(paras, stem=TRUE, remove_punct=TRUE, remove_symbols = TRUE, remove_numbers = TRUE, remove=c(stopwords("english")))
anthem_dtm <- dfm_remove(anthem_dtm, c("s","?","?",'thi'))
anthem_dtm <- dfm_trim(anthem_dtm, min_termfreq=20) # to trim
anthem_dtm
colnames(anthem_dtm)
# make a wordcloud
library("quanteda.textplots")
textplot_wordcloud(anthem_dtm,adjust=.6)
######################################################
# Here's where the LDA topic modeling occurs
library("topicmodels")
library('tidytext')
library('dplyr')
# get tdm matrix, anthem_dtm is a dfm, document feature matrix, which is the transpose of dtm
anthem_topics <- convert(anthem_dtm, to = "topicmodels") # same as tdm earlier
topic_model <- LDA(anthem_topics, method = "VEM", k=5)
terms(topic_model,5)
# Beta is the probability that a given term appears in a particular topic.
# Higher probability terms "define" the topic best.
tidy_topics <- tidy(topic_model, matrix = "beta")
tidy_topics
# no need to understand this as much!: can copy paste when needed. (true for
# rest of the file.)
anthem_top_topics <- tidy_topics %>%
group_by(topic) %>%
slice_max(beta, n = 2) %>% # cool func, gets the max n for each topic group
ungroup() %>% # to get the tibble without group tag
arrange(topic, -beta) # sort by topic, beta decreasing
anthem_top_topics %>%
mutate(term = reorder_within(term, beta, topic)) %>% # this hack is to order for facet
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~topic, scales = "free") + # scales="free" allows x-y scales to be free.
scale_y_reordered() # used in combo with reorder_within
# now the same for gamma
# Gamma is the probability that a given topic appears in a particular document.
# Higher probability documents show the main topics.
tidy_anthems <- tidy(topic_model, matrix = "gamma")
tidy_anthems
# try to name?
tidy_anthems <- tidy_anthems %>%
mutate(topicname = ifelse(topic==1, '1: glory,homeland',ifelse(topic==2,'2: god,nation',ifelse(topic==3,'3: land,love',ifelse(topic==4,'4: us,one',ifelse(topic==5,'5: may,eternal','.'))))))
# gamma plots
top_anthems <- tidy_anthems %>%
group_by(topicname) %>%
slice_max(gamma, n = 10) %>%
ungroup() %>%
arrange(document, -gamma)
top_anthems %>%
mutate(document = reorder_within(document, gamma, topicname)) %>%
ggplot(aes(gamma, document, fill = factor(topicname))) +
geom_col(show.legend = FALSE) +
facet_wrap(~topicname, scales = "free") +
scale_y_reordered() # takes care of labels
# amazing:
tidy_anthems %>% filter(document=='text129')
# let's make a map!
# we need country names!!
library('stringr') # to get part of string from column, for code
tidy_anthems <- tidy_anthems %>% mutate(countryid=as.numeric(str_extract(tidy_anthems$document, "[0-9]+"))) # gets code
anthems <- anthems %>%
mutate(countryid = factor(rownames(anthems))) # add code as new var
df <- merge(tidy_anthems,anthems,on='countryid')
library("rnaturalearth")
library("rnaturalearthdata")
library("sf")
world <- ne_countries(scale = "medium", returnclass = "sf") # to load world map
world$CountryCode = world$su_a3 # has bunch of codes
df$CountryCode = df$Alpha.3 # making sure df also has same code
set.seed(1)
perList <- NULL # We are going to make a list of perplexity values
# Loop: Try every number of topics from 2 up to maxTopics
for (i in 2:maxTopics) {
topic_model <- LDA(anthem_topics, method = "VEM", k=i)  # Latent Dirichlet Allocation
perList[i-1] <- perplexity(topic_model)
}
# Label and plot the perplexity list: Look for the knee
names(perList) <- as.character(2:maxTopics)
perList_df <- as.data.frame(perList)
library(ggplot2)
ggplot(data=perList_df,aes(x=as.numeric(rownames(perList_df)), y=perList)) + geom_point() + geom_line(linetype='dashed') + labs(x='num topics',y='perplexity')
topic_model <- LDA(anthem_topics, method = "VEM", k=3)
terms(topic_model,3)
# Beta is the probability that a given term appears in a particular topic.
# Higher probability terms "define" the topic best.
tidy_topics <- tidy(topic_model, matrix = "beta")
#gamma
tidy_anthems <- tidy(topic_model, matrix = "gamma")
tidy_anthems
# name etc, which can be skipped.
tidy_anthems <- tidy_anthems %>%
mutate(topicname = ifelse(topic==1, '1: glory,homeland',ifelse(topic==2,'2: god,nation',ifelse(topic==3,'3: land,love',ifelse(topic==4,'4: us,one',ifelse(topic==5,'5: may,eternal','.'))))))
top_anthems <- tidy_anthems %>%
group_by(topicname) %>%
slice_max(gamma, n = 10) %>%
ungroup() %>%
arrange(document, -gamma)
# How Many Documents for Each Topic?
top_anthems %>% group_by(topic) %>% summarize(count=n(),av_gamma = mean(gamma)) %>% ggplot(.,aes(x=topic,y=count)) + geom_col()
# How Many Documents for Each Topic?
top_anthems %>% group_by(topic) %>% summarize(count=n(),av_gamma = mean(gamma)) %>% ggplot(.,aes(x=topic,y=count)) + geom_col()
top_anthems %>%
mutate(document = reorder_within(document, gamma, topicname)) %>%
ggplot(aes(gamma, document, fill = factor(topicname))) +
geom_col(show.legend = FALSE) +
facet_wrap(~topicname, scales = "free") +
scale_y_reordered() # takes care of labels
