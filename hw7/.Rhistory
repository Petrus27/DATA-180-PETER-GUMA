Kmeans_3<-kmeans(Cluster_Ex,centers=10)
Kmeans_3
#Centers.
Kmeans_3$centers
#Within Group Sum of Squares.
Kmeans_3$withinss
#Clusters.
Kmeans_3$cluster
# Plotting clusters.
plot(X2~X1,data=Cluster_Ex,xlim=c(0,4),ylim=c(0,4),cex.axis=1.3, cex.lab=1.2,cex=1.2,pch=15+Kmeans_3$cluster,col=Kmeans_3$cluster)
#Adding centroids.
points(X2~X1,data=Kmeans_3$centers,pch=10,cex=1.8,col="blue")
Kmeans_3<-kmeans(Cluster_Ex,centers=3)
Kmeans_3
#Centers.
Kmeans_3$centers
#Within Group Sum of Squares.
Kmeans_3$withinss
#Clusters.
Kmeans_3$cluster
# Plotting clusters.
plot(X2~X1,data=Cluster_Ex,xlim=c(0,4),ylim=c(0,4),cex.axis=1.3, cex.lab=1.2,cex=1.2,pch=15+Kmeans_3$cluster,col=Kmeans_3$cluster)
#Adding centroids.
points(X2~X1,data=Kmeans_3$centers,pch=10,cex=1.8,col="blue")
palette()
Kmeans_5_1<-kmeans(Cluster_Ex,centers=5)
plot(X2~X1,data=Cluster_Ex,xlim=c(0,4),ylim=c(0,4),cex.axis=1.3, cex.lab=1.2,cex=1.2,pch=15+Kmeans_5_1$cluster,col=Kmeans_5_1$cluster)
Kmeans_5_2<-kmeans(Cluster_Ex,centers=5)
plot(X2~X1,data=Cluster_Ex,xlim=c(0,4),ylim=c(0,4),cex.axis=1.3, cex.lab=1.2,cex=1.2,pch=15+Kmeans_5_2$cluster,col=Kmeans_5_2$cluster)
Kmeans_5_1<-kmeans(Cluster_Ex,centers=5)
plot(X2~X1,data=Cluster_Ex,xlim=c(0,4),ylim=c(0,4),cex.axis=1.3, cex.lab=1.2,cex=1.2,pch=15+Kmeans_5_1$cluster,col=Kmeans_5_1$cluster)
Kmeans_5_2<-kmeans(Cluster_Ex,centers=5)
plot(X2~X1,data=Cluster_Ex,xlim=c(0,4),ylim=c(0,4),cex.axis=1.3, cex.lab=1.2,cex=1.2,pch=15+Kmeans_5_2$cluster,col=Kmeans_5_2$cluster)
#using set seed for reproducibility.
Kmeans_5_3<-kmeans(Cluster_Ex,centers=Cluster_Ex[c(2,5,7,21,36),])
set.seed(5)
Kmeans_5_4<-kmeans(Cluster_Ex,centers=5)
plot(X2~X1,data=Cluster_Ex,xlim=c(0,4),ylim=c(0,4),cex.axis=1.3, cex.lab=1.2,cex=1.2,pch=15+Kmeans_5_4$cluster,col=Kmeans_5_4$cluster)
set.seed(6)
Kmeans_5_4<-kmeans(Cluster_Ex,centers=5)
plot(X2~X1,data=Cluster_Ex,xlim=c(0,4),ylim=c(0,4),cex.axis=1.3, cex.lab=1.2,cex=1.2,pch=15+Kmeans_5_4$cluster,col=Kmeans_5_4$cluster)
# Clustering with 3 clusters, nstart = 1. Randomize only once.
Cluster3_S1<-kmeans(Cluster_Ex,centers=3)
Cluster3_S50<-kmeans(Cluster_Ex,centers=3,nstart=50)
Cluster3_S1
Cluster3_S1$withinss
set.seed(6)
#using set seed for reproducibility.
Kmeans_5_3<-kmeans(Cluster_Ex,centers=Cluster_Ex[c(2,5,7,21,36),])
set.seed(6)
Kmeans_5_4<-kmeans(Cluster_Ex,centers=5)
plot(X2~X1,data=Cluster_Ex,xlim=c(0,4),ylim=c(0,4),cex.axis=1.3, cex.lab=1.2,cex=1.2,pch=15+Kmeans_5_4$cluster,col=Kmeans_5_4$cluster)
set.seed(5)
Kmeans_5_4<-kmeans(Cluster_Ex,centers=5)
plot(X2~X1,data=Cluster_Ex,xlim=c(0,4),ylim=c(0,4),cex.axis=1.3, cex.lab=1.2,cex=1.2,pch=15+Kmeans_5_4$cluster,col=Kmeans_5_4$cluster)
# Clustering with 3 clusters, nstart = 1. Randomize only once.
Cluster3_S1<-kmeans(Cluster_Ex,centers=3)
Cluster3_S50<-kmeans(Cluster_Ex,centers=3,nstart=50)
Cluster3_S1
Cluster3_S1$withinss
Cluster3_S1$tot.withinss
# Clustering with 3 clusters, nstart =50. Randomize 50 times.
Cluster3_S50
Cluster3_S50$withinss
Cluster3_S50$tot.withinss
Cluster3_S50$betweenss
# WGSS as a function of the number of clusters for values of K from 1 to 8.
wgss<-rep(0,8)
for(i in 1:8){wgss[i]<-kmeans(Cluster_Ex,centers=i,nstart=50)$tot.withinss}
plot(c(1:8),wgss,type="b",pch=16,cex=1.3,ylim=c(0,100), xlab="Number of Groups",ylab="WGSS")
# Clustering with 3 clusters, nstart =50. Randomize 50 times.
Cluster3_S50
Cluster3_S50$withinss
Cluster3_S50$tot.withinss
Cluster3_S50$betweenss
# WGSS as a function of the number of clusters for values of K from 1 to 8.
wgss<-rep(0,8)
for(i in 1:8){wgss[i]<-kmeans(Cluster_Ex,centers=i,nstart=50)$tot.withinss}
plot(c(1:8),wgss,type="b",pch=16,cex=1.3,ylim=c(0,100), xlab="Number of Groups",ylab="WGSS")
Kmeans_3$betweenss
# Clustering with 3 clusters, nstart = 1. Randomize only once.
Cluster3_S1<-kmeans(Cluster_Ex,centers=3)
Cluster3_S50<-kmeans(Cluster_Ex,centers=3,nstart=50)
Cluster3_S1
Cluster3_S1$withinss
Cluster3_S1$tot.withinss
# Clustering with 3 clusters, nstart =50. Randomize 50 times.
Cluster3_S50
Cluster3_S50$withinss
Cluster3_S50$tot.withinss
Cluster3_S50$betweenss
# WGSS as a function of the number of clusters for values of K from 1 to 8.
wgss<-rep(0,8)
for(i in 1:8){wgss[i]<-kmeans(Cluster_Ex,centers=i,nstart=50)$tot.withinss}
plot(c(1:8),wgss,type="b",pch=16,cex=1.3,ylim=c(0,100), xlab="Number of Groups",ylab="WGSS")
Kmeans_3$betweenss
# CH index.
chindex=c() # or rep(0,7)
# CH index.
chindex=c() # or rep(0,7)
i=1
for(k in 2:8){
chindex[i] <- (kmeans(df[2:3],k)$betweenss/(k-1))  / (kmeans(df[2:3],k)$tot.withinss/((nrow(df)-k)))
i=i+1
}
# CH index.
chindex=c() # or rep(0,7)
# CH index.
chindex=c() # or rep(0,7)
i=1
for(k in 2:8){
chindex[i] <- (kmeans(df[2:3],k)$betweenss/(k-1))  / (kmeans(df[2:3],k)$tot.withinss/((nrow(df)-k)))
i=i+1
}
plot(2:8,chindex,pch=19,type='b')
for(i in 1:8){wgss[i]<-kmeans(Cluster_Ex,centers=i,nstart=50)$tot.withinss}
# WGSS as a function of the number of clusters for values of K from 1 to 8.
wgss<-rep(0,8)
for(i in 1:8){wgss[i]<-kmeans(Cluster_Ex,centers=i,nstart=50)$tot.withinss}
plot(c(1:8),wgss,type="b",pch=16,cex=1.3,ylim=c(0,100), xlab="Number of Groups",ylab="WGSS")
news <- news %>% group_by(year,month) %>% mutate(count=n(), yearmonth = paste(year, month,sep = '/')) %>% arrange(year,month,day)
library(tidyverse)
library(dplyr) # I added this so it would knit properly
library(tm)
setwd("C:/Users/Peter/Documents/College Documents/DATA 180/Data 180 Peter Guma/DATA-180-PETER-GUMA/hw7")
news<-read.csv("news.csv",header=T)
posWords <- scan("positive-words.txt", character(0), sep = "\n")  # 2006 items
negWords <- scan("negative-words.txt", character(0), sep = "\n")  # 4783 items
head(posWords,15)
head(negWords,15)
nrow(news)
charVector <- news$headline_text
head(charVector)
wordVector <- VectorSource(charVector)
wordCorpus <- Corpus(wordVector)
wordCorpus <- tm_map(wordCorpus, tolower)
wordCorpus <- tm_map(wordCorpus, removePunctuation)
wordCorpus <- tm_map(wordCorpus, removeNumbers)
wordCorpus <- tm_map(wordCorpus, removeWords, stopwords("en"))
tdm <- TermDocumentMatrix(wordCorpus)
m <- as.matrix(tdm)
wordCounts <- rowSums(m)
sortedWordCounts <- sort(wordCounts, decreasing = TRUE)
head(sortedWordCounts, n=10)
barplot(sortedWordCounts[sortedWordCounts>=50], ylab="Frequency", las=2, cex.names=.75, ylim=c(0,350), main="Words Appearing 50+ Times in News Headlines")
totalUnique = nrow(unique(m))
num_pos <- 0
num_neg <- 0
pos_match <- match(names(wordCounts), posWords, nomatch = 0)
pos_match <- pos_match != 0
pos_match <- wordCounts[pos_match]
pos_match
neg_match <- match(names(wordCounts), negWords, nomatch = 0)
neg_match <- neg_match != 0
neg_match <- wordCounts[neg_match]
neg_match
pct_pos <- sum(pos_match)/totalUnique
pct_neg <- sum(neg_match)/totalUnique
barplot(sort(pos_match[pos_match>=20], decreasing=TRUE), ylab="Frequency", las=2, ylim=c(0,80), main="Positive Words Appearing 20+ Times in Headlines")
barplot(sort(neg_match[neg_match>=20], decreasing=TRUE), ylab="Frequency", las=2, ylim=c(0,120), main="Negative Words Appearing 20+ Times in Headlines")
barplot(sort(pos_match[pos_match>=20], decreasing=TRUE), ylab="Frequency", las=2, ylim=c(0,80), main="Positive Words Appearing 20+ Times in Headlines")
barplot(sort(neg_match[neg_match>=20], decreasing=TRUE), ylab="Frequency", las=2, ylim=c(0,120), main="Negative Words Appearing 20+ Times in Headlines")
barplot(sort(pos_match[pos_match>=20], decreasing=TRUE), ylab="Frequency", las=2, ylim=c(0,80), main="Positive Words Appearing 20+ Times in Headlines")
barplot(sort(neg_match[neg_match>=20], decreasing=TRUE), ylab="Frequency", las=2, ylim=c(0,120), main="Negative Words Appearing 20+ Times in Headlines")
```
news <- news %>% group_by(year,month) %>% mutate(count=n(), yearmonth = paste(year, month,sep = '/')) %>% arrange(year,month,day)
ggplot(news, aes(x=factor(yearmonth, levels = unique(yearmonth)))) + geom_bar() + theme(axis.text=element_text(size=4,angle=90))
news$yearmonth
head(news)
library("quanteda")
library("quanteda")
library('corpus')
library("corpus")
library("corpus")
term_stats()
library("corpus")
library("corpus")
library("tm_map")
library("tmap")
library("tmap")
library("corpus")
library("corpus")
library("corpus")
library("tmap")
install.packages("corpus")
library("corpus")
install.packages("corpus")
library("corpus")
install.packages("corpus")
View(news)
newCharVector <- news$headline_text
newWordVector <- VectorSource(newCharVector)
newWordCorpus <- Corpus(newWordVector)
wordCorpus <- tm_map(newWordCorpus, tolower)
wordCorpus <- tm_map(newWordCorpus, removePunctuation)
wordCorpus
head(wordCorpus)
View(wordCorpus)
dfm <- dfm(newWordCorpus, remove=stopwords("en"), remove_punct=TRUE, remove_symbols=TRUE)
newCharVector <- news$headline_text
newWordVector <- VectorSource(newCharVector)
newWordCorpus <- Corpus(newWordVector)
dfm <- dfm(newWordCorpus, remove=stopwords("en"), remove_punct=TRUE, remove_symbols=TRUE)
news <- news %>% group_by(year,month) %>% mutate(count=n(), yearmonth = paste(year, month,sep = '/')) %>% arrange(year,month,day)
ggplot(news, aes(x=factor(yearmonth, levels = unique(yearmonth)))) + geom_bar() + theme(axis.text=element_text(size=4,angle=90))
library("quanteda")
dfm <- dfm(news, remove=stopwords("en"), remove_punct=TRUE, remove_symbols=TRUE)
dfm <- dfm(corpus(news), remove=stopwords("en"), remove_punct=TRUE, remove_symbols=TRUE)
dfm <- dfm(x=corpus(news), remove=stopwords("en"), remove_punct=TRUE, remove_symbols=TRUE)
newCharVector <- news$headline_text
newWordVector <- VectorSource(newCharVector)
newWordCorpus <- Corpus(newWordVector)
dfm <- dfm(newWordCorpus, remove=stopwords("en"), remove_punct=TRUE, remove_symbols=TRUE)
dfm <- dfm(newCharVector, remove=stopwords("en"), remove_punct=TRUE, remove_symbols=TRUE)
View(dfm)
View(dfm)
head(dfm)
head(dfm[dfm>20])
head(dfm[dfm>2])
head(dfm[dfm>1])
topfeatures(dfm, n=20)
dfm <- dfm(newCharVector, remove=stopwords("en"), remove_punct=TRUE, remove_symbols=TRUE)
topfeatures(dfm, n=20)
dfm2 <- dfm(corpus(newCharVector, tokens(remove=stopwords("en"), remove_punct=TRUE, remove_symbols=TRUE), tokens_ngrams(n=2)))
dfm2 <- dfm(corpus(newCharVector), tokens(remove=stopwords("en"), remove_punct=TRUE, remove_symbols=TRUE), tokens_ngrams(n=2))
dfm2 <- dfm(tokens(newCharVector), tokens(remove=stopwords("en"), remove_punct=TRUE, remove_symbols=TRUE), tokens_ngrams(n=2))
dfm2 <- dfm(corpus(newCharVector), tokens(remove=stopwords("en"), remove_punct=TRUE, remove_symbols=TRUE), tokens_ngrams(n=2))
dfm2 <- dfm(newCharVector, tokens(remove=stopwords("en"), remove_punct=TRUE, remove_symbols=TRUE), tokens_ngrams(n=2))
dfm2 <- dfm(tokens(newCharVector, remove=stopwords("en"), remove_punct=TRUE, remove_symbols=TRUE), tokens_ngrams(n=2))
dfm2 <- dfm(tokens(newCharVector, remove_stopwords=TRUE, remove_punct=TRUE, remove_symbols=TRUE), tokens_ngrams(n=2))
dfm2 <- dfm(newCharVector, remove=stopwords("en"))
dfm2 <- dfm2(tokens(dfm2, remove_punct=TRUE, remove_symbols=TRUE), tokens_ngrams(n=2))
dfm2 <- dfm(newCharVector, remove=stopwords("en"))
dfm2 <- dfm2(tokens(dfm2, remove_punct=TRUE, remove_symbols=TRUE), tokens_ngrams(n=2))
dfm2 <- dfm2(tokens(newCharVector, remove_punct=TRUE, remove_symbols=TRUE), tokens_ngrams(n=2))
dfm2 <- dfm(tokens(dfm2, remove_punct=TRUE, remove_symbols=TRUE), tokens_ngrams(n=2))
dfm2 <- tm_map(newCharVector, removeWords, stopwords("en"))
dfm2 <- dfm(tokens(newCharVector, remove_punct=TRUE, remove_symbols=TRUE), tokens_ngrams(n=2))
dfm2 <- dfm(newCharVector, tokens(newCharVector, remove_punct=TRUE, remove_symbols=TRUE), tokens_ngrams(n=2))
dfm2 <- dfm(newCharVector, tokens(remove_punct=TRUE, remove_symbols=TRUE), tokens_ngrams(n=2))
dfm2 <- dfm(newCharVector, tokens(newCharVector, remove_punct=TRUE, remove_symbols=TRUE), tokens_ngrams(n=2))
newWordVector <- VectorSource(newCharVector)
newWordCorpus <- Corpus(newWordVector)
dfm2 <- tm_map(newWordCorpus, removeWords, stopwords("en"))
View(dfm2)
library("quanteda")
news <- news %>% group_by(year,month) %>% mutate(count=n(), yearmonth = paste(year, month,sep = '/')) %>% arrange(year,month,day)
ggplot(news, aes(x=factor(yearmonth, levels = unique(yearmonth)))) + geom_bar() + theme(axis.text=element_text(size=4,angle=90))
dfm <- dfm(newCharVector, remove=stopwords("en"), remove_punct=TRUE, remove_symbols=TRUE)
topfeatures(dfm, n=20)
newWordVector <- VectorSource(newCharVector)
newWordCorpus <- Corpus(newWordVector)
dfm2 <- tm_map(newWordCorpus, remove=stopwords("en"))
dfm2 <- tm_map(newWordCorpus, removeWords, stopwords("en"))
# dfm2 <- dfm(newCharVector, remove=stopwords("en"))
dfm2 <- dfm(dfm2, tokens(newCharVector, remove_punct=TRUE, remove_symbols=TRUE), tokens_ngrams(n=2))
# dfm2 <- dfm(newCharVector, remove=stopwords("en"))
dfm2 <- dfm(newCharVector, tokens(newCharVector, remove_punct=TRUE, remove_symbols=TRUE), tokens_ngrams(n=2))
dfm2 <- dfm(newWordCorpus, tokens(newCharVector, remove_punct=TRUE, remove_symbols=TRUE), tokens_ngrams(n=2))
newWordCorpus <- Corpus(newWordVector)
dfm2 <- dfm(newWordCorpus, tokens(newCharVector, remove_punct=TRUE, remove_symbols=TRUE), tokens_ngrams(n=2))
newWordCorpus <- corpus(newWordVector)
newWordVector <- VectorSource(newCharVector)
newWordVector <- Corpus(newCharVector)
newWordVector <- VectorSource(newCharVector)
newWordCorpus <- Corpus(newWordVector)
dfm2 <- dfm(newWordCorpus, tokens(newCharVector, remove_punct=TRUE, remove_symbols=TRUE), tokens_ngrams(n=2))
dfm2 <- tm_map(corpus(newCharVector), removeWords, stopwords("en"))
dfm2 <- newCharVector %>%
corpus() %>%
tokens(remove_punct = TRUE,
remove_numbers = TRUE) %>%
tokens_ngrams(n = 2) %>%
dfm() %>%
dfm_trim(min_docfreq = 5)
topfeatures(dfm2, n=20)
dfm2 <- dfm(newWordCorpus, tokens(newCharVector, remove_punct=TRUE, remove_symbols=TRUE), tokens_ngrams(n=2))
dfm2 <- newCharVector %>%
corpus() %>%
tokens(remove_punct = TRUE,
remove_numbers = TRUE) %>%
tokens_ngrams(n = 2) %>%
dfm()
dfm2 <- newCharVector %>%
corpus() %>%
tokens(remove_punct = TRUE,
remove_numbers = TRUE) %>%
tokens_ngrams(n = 2) %>%
dfm() %>%
dfm_trim(min_docfreq = 5)
dfm2 <- newCharVector %>%
corpus() %>%
tokens(remove_punct = TRUE,
remove_numbers = TRUE) %>%
tokens_ngrams(n = 2) %>%
dfm()
dfm2 <- newCharVector %>%
corpus() %>%
tokens(remove_symbols=TRUE, remove_punct=TRUE) %>%
tokens_ngrams(n = 2) %>%
dfm()
topfeatures(dfm2, n=20)
View(dfm2)
dfm2 <- newCharVector %>%
corpus() %>%
tokens(remove=stopwords("en"), remove_symbols=TRUE, remove_punct=TRUE) %>%
tokens_ngrams(n = 2) %>%
dfm()
dfm <- dfm(newCharVector, remove=stopwords("en"), remove_punct=TRUE, remove_symbols=TRUE)
topfeatures(dfm, n=20)
dfm2 <- newCharVector %>%
corpus() %>%
tokens(remove_symbols=TRUE, remove_punct=TRUE) %>%
tokens_ngrams(n = 2) %>%
dfm()
topfeatures(dfm2, n=20)
head(dfm2)
dfm2 <- dfm(dfm2, remove=stopwords("en"))
head(dfm2)
topfeatures(dfm2, n=20)
dfm2 <- dfm(dfm2, remove=stopwords("en"))
topfeatures(dfm2, n=20)
dfm2 <- dfm(dfm2, remove=stopwords("en"))
dfm2 <- newCharVector %>%
corpus() %>%
tokens(remove_symbols=TRUE, remove_punct=TRUE) %>%
tokens_ngrams(n = 2) %>%
dfm()
dfm <- dfm(newCharVector, remove=stopwords("en"), remove_punct=TRUE, remove_symbols=TRUE)
topfeatures(dfm, n=20)
dfm <- dfm(newCharVector, remove=stopwords("en"), remove_punct=TRUE, remove_symbols=TRUE)
dfm2 <- newCharVector %>%
corpus() %>%
tokens(remove_symbols=TRUE, remove_punct=TRUE) %>%
tokens_ngrams(n = 2) %>%
dfm()
dfm2 <- corpus(newCharVector) %>%
tokens(remove_symbols=TRUE, remove_punct=TRUE) %>%
tokens_ngrams(n = 2) %>%
dfm()
dfm2 <- dfm(dfm2, remove=stopwords("en"))
topfeatures(dfm2, n=20)
topfeatures(dfm, n=20)
dfm2 <- corpus(newCharVector) %>%
tokens(remove=stopwords("en"), remove_symbols=TRUE, remove_punct=TRUE) %>%
tokens_ngrams(n = 2) %>%
dfm()
dfm2 <- corpus(newCharVector) %>%
tokens(pattern=stopwords("en"), remove_symbols=TRUE, remove_punct=TRUE) %>%
tokens_ngrams(n = 2) %>%
dfm()
dfm2 <- corpus(newCharVector) %>%
tokens_select(pattern=stopwords("en"), remove_symbols=TRUE, remove_punct=TRUE) %>%
tokens_ngrams(n = 2) %>%
dfm()
dfm2 <- corpus(newCharVector) %>%
tokens(tokens_select(pattern=stopwords("en")), remove_symbols=TRUE, remove_punct=TRUE) %>%
tokens_ngrams(n = 2) %>%
dfm()
dfm2 <- tokens_select(corpus(newCharVector), pattern=stopwords("en")) %>%
tokens(remove_symbols=TRUE, remove_punct=TRUE) %>%
tokens_ngrams(n = 2) %>%
dfm()
dfm2 <- tokens_select(tokens(corpus(newCharVector)), pattern=stopwords("en")) %>%
tokens(remove_symbols=TRUE, remove_punct=TRUE) %>%
tokens_ngrams(n = 2) %>%
dfm()
topfeatures(dfm2, n=20)
dfm2 <- tokens_select(tokens(corpus(newCharVector), remove_symbols=TRUE, remove_punct=TRUE), pattern=stopwords("en")) %>%
tokens(remove_symbols=TRUE, remove_punct=TRUE) %>%
tokens_ngrams(n = 2) %>%
dfm()
topfeatures(dfm2, n=20)
dfm2 <- tokens_select(tokens(corpus(newCharVector), remove_symbols=TRUE, remove_punct=TRUE), pattern=stopwords("en")) %>%
tokens_ngrams(n = 2) %>%
dfm()
topfeatures(dfm2, n=20)
dfm2 <- tokens_select(tokens(corpus(newCharVector), remove_symbols=TRUE, remove_punct=TRUE), pattern=stopwords("en"), selection="remove") %>%
tokens_ngrams(n = 2) %>%
dfm()
topfeatures(dfm2, n=20)
news <- news %>% group_by(year,month) %>% mutate(count=n(), yearmonth = paste(year, month,sep = '/')) %>% arrange(year,month,day)
library("quanteda")
dfm <- dfm(newCharVector, remove=stopwords("en"), remove_punct=TRUE, remove_symbols=TRUE)
topfeatures(dfm, n=20)
dfm2 <- tokens_select(tokens(corpus(newCharVector), remove_symbols=TRUE, remove_punct=TRUE), pattern=stopwords("en"), selection="remove") %>%
tokens_ngrams(n = 2) %>%
dfm()
topfeatures(dfm2, n=20)
newscorpus <- corpus(news)
newscorpus <- corpus(newCharVector)
corpus_reshape(newscorpus)
newscorpus <- corpus(newCharVector)
corpus_reshape(newscorpus)
newscorpus <- corpus(newCharVector)
head(newscorpus)
corpus_reshape(newscorpus)
head(newscorpus)
newscorpus <- corpus(newCharVector)
head(newscorpus)
corpus_reshape(newscorpus)
head(newscorpus)
corpus_reshape(newscorpus, to="paragraphs")
head(newscorpus)
newscorpus <- corpus(newCharVector)
```
news <- news %>% group_by(year,month) %>% mutate(count=n(), yearmonth = paste(year, month,sep = '/')) %>% arrange(year,month,day)
```
```
newCharVector <- news$headline_text
```
newCharVector <- news$headline_text
newCharVector <- news$headline_text
```
```
dfm <- dfm(newCharVector, remove=stopwords("en"), remove_punct=TRUE, remove_symbols=TRUE)
topfeatures(dfm, n=20)
```
```
```
newscorpus <- corpus(newCharVector)
head(newscorpus)
corpus_reshape(newscorpus, to="paragraphs")
head(newscorpus)
newscorpus <- corpus(newCharVector)
head(newscorpus)
newscorpus <- corpus_reshape(newscorpus, to="paragraphs")
head(newscorpus)
newscorpus <- corpus(newCharVector)
newscorpus <- corpus_reshape(newscorpus, to="paragraphs")
head(newscorpus)
newscorpus <- corpus(newCharVector)
newscorpus <- corpus_reshape(newscorpus, to="paragraphs")
head(newscorpus)
newscorpus <- corpus_reshape(corpus(newCharVector), to="paragraphs")
head(newscorpus)
newscorpus <- corpus_reshape(newCharVector, to="paragraphs")
newscorpus <- corpus(newCharVector)
corpus(newCharVector)
corpus(newCharVector)
newscorpus <- corpus(newCharVector)
newscorpus <- corpus_reshape(newscorpus, to="paragraphs")
corpus(newCharVector)
newscorpus
head(newscorpus)
newscorpus <- corpus(newCharVector)
head(newscorpus)
newscorpus <- corpus_reshape(newscorpus, to="paragraphs")
head(newscorpus)
newscorpus <- corpus_reshape(newscorpus, to="sentences")
head(newscorpus)
newscorpus <- corpus_reshape(newscorpus, to="paragraphs")
head(newscorpus)
newscorpus <- corpus(news)
newscorpus <- corpus(news$headline_text)
newscorpus <- corpus_reshape(newscorpus, to="paragraphs")
head(newscorpus)
newscorpus <- corpus(newCharVector)
newscorpus <- corpus(news)
newscorpus <- corpus(news$headline_text)
newscorpus <- corpus_reshape(newscorpus, to="paragraphs")
head(newscorpus)
newscorpus <- corpus(newCharVector)
newscorpus <- corpus_reshape(newscorpus, to="paragraphs")
head(newscorpus)
newscorpus <- corpus(newCharVector)
newscorpus <- corpus_reshape(newscorpus, to="paragraphs") #unclear why this doesn't do anything
head(newscorpus)
news_dtm <- dtm(newscorpus)
news_dfm <- dfm(newscorpus)
news_dfm <- dfm <- dfm(newCharVector, remove=stopwords("en"), remove_punct=TRUE, remove_symbols=TRUE)
news_dfm <- dfm(newCharVector, remove=stopwords("en"), remove_punct=TRUE, remove_symbols=TRUE, remove_numbers=TRUE)
news_dfm
head(news_dfm)
news_dfm <- news_dfm[news_dfm>=50]
head(news_dfm)
news_dfm <- dfm(newCharVector, remove=stopwords("en"), remove_punct=TRUE, remove_symbols=TRUE, remove_numbers=TRUE)
head(news_dfm)
View(news_dfm)
View(news_dfm)
news_dfm <- news_dfm[news_dfm$features>=50]
news_dfm <- news_dfm[>=50]
news_dfm <- dfm(newCharVector, remove=stopwords("en"), remove_punct=TRUE, remove_symbols=TRUE, remove_numbers=TRUE)
news_dtm <- dfm(newCharVector, remove=stopwords("en"), remove_punct=TRUE, remove_symbols=TRUE, remove_numbers=TRUE)
news_dtm <- tokens(corpus_subset(news_dtm, news_dtm[news_dtm>=50]))
news_dtm <- tokens(corpus_subset(corpus(news_dtm), news_dtm[news_dtm>=50]))
news_dtm <- tokens(corpus_subset(Corpus(news_dtm), news_dtm[news_dtm>=50]))
news_dtm <- dfm(newCharVector, remove=stopwords("en"), remove_punct=TRUE, remove_symbols=TRUE, remove_numbers=TRUE)
# news_dtm <- tokens(corpus_subset(corpus(news_dtm), news_dtm[news_dtm>=50]))
# news_dtm <- news_dtm[>=50]
head(news_dtm)
# news_dtm <- tokens(corpus_subset(corpus(news_dtm), news_dtm[news_dtm>=50]))
# news_dtm <- news_dtm[>=50]
news_dtm <- dfm_subset(news_dtm, news_dtm[news_dtm>=50])
head(news_dtm)
news_dtm <- dfm(newCharVector, remove=stopwords("en"), remove_punct=TRUE, remove_symbols=TRUE, remove_numbers=TRUE)
# news_dtm <- tokens(corpus_subset(corpus(news_dtm), news_dtm[news_dtm>=50]))
# news_dtm <- news_dtm[>=50]
news_dtm <- dfm_subset(news_dtm, news_dtm[])
news_dtm <- dfm(newCharVector, remove=stopwords("en"), remove_punct=TRUE, remove_symbols=TRUE, remove_numbers=TRUE)
# news_dtm <- tokens(corpus_subset(corpus(news_dtm), news_dtm[news_dtm>=50]))
# news_dtm <- news_dtm[>=50]
news_dtm <- dfm_subset(news_dtm, news_dtm)
# news_dtm <- tokens(corpus_subset(corpus(news_dtm), news_dtm[news_dtm>=50]))
# news_dtm <- news_dtm[>=50]
news_dtm <- dfm_subset(news_dtm)
head(news_dtm)
news_dtm <- dfm(newCharVector, remove=stopwords("en"), remove_punct=TRUE, remove_symbols=TRUE, remove_numbers=TRUE)
# news_dtm <- tokens(corpus_subset(corpus(news_dtm), news_dtm[news_dtm>=50]))
# news_dtm <- news_dtm[>=50]
# news_dtm <- dfm_subset(news_dtm) # unsure how to manipulate this
head(news_dtm)
